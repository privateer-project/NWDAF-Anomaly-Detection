import logging

from pathlib import Path

import mlflow
import torch

def load_model_weights(model_path: str, paths_config) -> dict:
    """
    Load model weights from various path formats.

    Args:
        model_path: Path to model file or experiment directory
        paths_config: Paths configuration

    Returns:
        Model state dictionary
    """
    model_path = Path(model_path)

    # Try different path resolutions
    possible_paths = [
        model_path,  # Direct path
        paths_config.experiments_dir / model_path / 'model.pt',  # Experiment directory
        paths_config.models_dir / model_path,  # Models directory
        Path(model_path).with_suffix('.pt'),  # Add .pt extension
    ]

    for path in possible_paths:
        if path.exists():
            logging.info(f'Loading model from: {path}')
            try:
                state_dict = torch.load(path, map_location='cpu')

                # Handle different state dict formats
                if isinstance(state_dict, dict):
                    # Remove common prefixes from distributed training
                    cleaned_state_dict = {}
                    for key, value in state_dict.items():
                        # Remove common prefixes
                        clean_key = key
                        for prefix in ['_module.', 'module.']:
                            if clean_key.startswith(prefix):
                                clean_key = clean_key[len(prefix):]
                        cleaned_state_dict[clean_key] = value

                    return cleaned_state_dict
                else:
                    raise ValueError(f'Unexpected state dict format: {type(state_dict)}')

            except Exception as e:
                logging.warning(f'Failed to load from {path}: {e}')
                continue

    raise FileNotFoundError(f'Could not find model file at any of: {[str(p) for p in possible_paths]}')


def log_model(model, model_name, dataloader, direction, target_metric, current_target_metric, experiment_id, pip_requirements):
    """Log trained model to MLFlow with proper signature and champion tagging."""
    model.to('cpu')
    signature = get_signature(model=model, dataloader=dataloader)

    client = mlflow.tracking.MlflowClient()

    # Determine sort direction for finding best run
    if direction == 'maximize':
        sorting = 'DESC'
    else:
        sorting = 'ASC'

    # Check if this is a new champion
    is_champion = True
    previous_champion_version = None

    try:
        # Find the best run across all experiments
        best_run = client.search_runs(experiment_id, order_by=[f'metrics.`{target_metric}` {sorting}'], max_results=1)[0]

        if target_metric in best_run.data.metrics:
            best_target_metric = best_run.data.metrics[target_metric]

            # Compare with previous best
            if direction == 'maximize':
                is_champion = current_target_metric >= best_target_metric
            else:
                is_champion = current_target_metric <= best_target_metric

            print(f'Previous best {target_metric}: {best_target_metric}')
            print(f'Current {target_metric}: {current_target_metric}')
            print(f'Is new champion: {is_champion}')
        else:
            print('Metric not found in best run. Skipping champion tagging.')

        # Find previous champion version to remove tag
        if is_champion:
            try:
                registered_model = client.get_registered_model(model_name)
                for version in registered_model.latest_versions:
                    version_details = client.get_model_version(model_name, version.version)
                    if version_details.tags.get('champion') == 'true':
                        previous_champion_version = version.version
                        break
            except Exception as e:
                print(f"No previous champion found or error accessing registry: {e}")

    except (IndexError, KeyError) as e:
        print(f"No previous runs found or error accessing metrics: {e}")
        # This is likely the first model, so it's automatically the champion
        is_champion = True

    # Log the model
    model_info = mlflow.pytorch.log_model(
        pytorch_model=model,
        artifact_path=model_name,
        registered_model_name=model_name,
        signature=signature,
        pip_requirements=pip_requirements)
    # Get the version number of the newly logged model
    current_version = model_info.registered_model_version

    # Handle champion tagging
    if is_champion:
        print(f"ðŸ† New champion model! Logging with champion tag.")

        # Remove champion tag from previous version
        if previous_champion_version:
            try:
                client.delete_model_version_tag(model_name, previous_champion_version, 'champion')
                print(f"Removed champion tag from version {previous_champion_version}")
            except Exception as e:
                print(f"Error removing champion tag from previous version: {e}")

        try:
            client.set_model_version_tag(model_name, current_version, 'champion', 'true')

            # Also add metadata about when it became champion
            import datetime
            client.set_model_version_tag(
                model_name,
                current_version,
                'champion_since',
                datetime.datetime.now().isoformat()
            )

            print(f"Added champion tag to version {current_version}")
        except Exception as e:
            print(f"Error adding champion tag: {e}")
    else:
        print(f"Model performance ({current_target_metric}) did not exceed"
              f" previous best ({best_target_metric}). No champion tag added.")

    client.set_model_version_tag(
        model_name,
        current_version,
        target_metric,
        str(current_target_metric)
    )

def get_signature(model, dataloader):
    _input = next(iter(dataloader))[0]['encoder_cont'][:1].to('cpu')
    _output = model(_input)

    if isinstance(_output, dict):
        _output = {key: val.detach().numpy() for key, val in _output.items()}
    else:
        _output = _output.detach().numpy()
    signature = mlflow.models.infer_signature(model_input=_input.detach().numpy(), model_output=_output)
    return signature


def load_champion_model(tracking_uri, model_name: str = "TransformerAD"):
    """Simple function to load your champion model."""
    "http://localhost:5001"
    mlflow.tracking.set_tracking_uri(tracking_uri)
    client = mlflow.tracking.MlflowClient()

    # Get registered model
    registered_model = client.get_registered_model(model_name)

    # Find champion version
    for version in registered_model.latest_versions:
        version_details = client.get_model_version(model_name, version.version)
        if version_details.tags.get('champion') == 'true':
            # Load the champion model
            model_uri = f"models:/{model_name}/{version.version}"
            model = mlflow.pytorch.load_model(model_uri)
            print(f"ðŸ† Loaded champion model v{version.version}")
            return model

    # If no champion found, load latest
    latest_version = registered_model.latest_versions[0].version
    model_uri = f"models:/{model_name}/{latest_version}"
    model = mlflow.pytorch.load_model(model_uri)
    print(f"âš ï¸ No champion found, loaded latest v{latest_version}")
    return model

# Usage - this is all you need!
if __name__ == '__main__':
    champion_model: torch.nn.Module = load_champion_model("TransformerAD")
